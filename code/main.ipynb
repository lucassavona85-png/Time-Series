{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fef3ec",
   "metadata": {},
   "source": [
    "# NOTEBOOK : TIME SERIES ANALYSIS \n",
    "## By Lea PONS, Morgan VIROLAN, Lucas SAVONA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#==============================================================================\n",
    "#LIBRARY IMPORTS\n",
    "#==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, filtfilt, freqz, welch\n",
    "from numpy.fft import rfft, rfftfreq\n",
    "from statsmodels.tsa.stattools import acf, pacf,adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.signal import buttord, butter, detrend\n",
    "import warnings\n",
    "\n",
    "#==================================================================================\n",
    "# LOAD DATA, PROCESS BIA, CREATE WINDOWS\n",
    "#==================================================================================\n",
    "\n",
    "#---------------\n",
    "# CONFIG \n",
    "#---------------\n",
    "\n",
    "#-------Data-------\n",
    "#Data paths and columns\n",
    "BIA_PKL_PATH = \"../data/LEA_BIA_RAW.pkl\"   # pkl file exported from BIA device (raw data for pandas)\n",
    "FREQ_COL = \"f_48800\"              # ~48.8 kHz complex impedance column\n",
    "\n",
    "# Window timestamps (BIA datetime): 3 minutes BEFORE / fatigue inducing protocole / and 3 minutes AFTER\n",
    "PRE_START_TIME  = \"2025-11-28 14:57:02.563\"\n",
    "PRE_END_TIME    = \"2025-11-28 15:00:02.563\"\n",
    "POST_START_TIME = \"2025-11-28 15:05:45.278\"\n",
    "POST_END_TIME   = \"2025-11-28 15:08:44.534\"\n",
    "\n",
    "# LOAD RAW BIA (.pkl)\n",
    "data_bia_raw = pd.read_pickle(BIA_PKL_PATH)\n",
    "\n",
    "# ANALYSIS DATAFRAME (same data, cleaner columns)\n",
    "data_bia = data_bia_raw.copy()\n",
    "\n",
    "# Parse time\n",
    "data_bia[\"time\"] = pd.to_datetime(data_bia[\"timestamp\"], errors=\"coerce\")\n",
    "data_bia = data_bia.dropna(subset=[\"time\"]).sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "# Complex impedance at ~48.8 kHz\n",
    "data_bia[\"Z_48.8k\"] = data_bia[FREQ_COL].astype(np.complex128)\n",
    "\n",
    "# Compute R, Xc, PhA (standard convention: Xc = -imag(Z))\n",
    "data_bia[\"R_48.8k_ohm\"]   = np.real(data_bia[\"Z_48.8k\"])\n",
    "data_bia[\"Xc_48.8k_ohm\"]  = -np.imag(data_bia[\"Z_48.8k\"])\n",
    "data_bia[\"PhA_48.8k_deg\"] = np.degrees(np.arctan2(data_bia[\"Xc_48.8k_ohm\"], data_bia[\"R_48.8k_ohm\"]))\n",
    "\n",
    "# Compact analysis view\n",
    "analysis_cols = [\"time\", \"Z_48.8k\", \"R_48.8k_ohm\", \"Xc_48.8k_ohm\", \"PhA_48.8k_deg\", \"sat\", \"min\", \"max\"]\n",
    "data_bia_analysis = data_bia[analysis_cols].copy()\n",
    "\n",
    "#----------------Windows---------------\n",
    "# CREATE WINDOWS (PRE / POST)\n",
    "\n",
    "pre_start  = pd.to_datetime(PRE_START_TIME)\n",
    "pre_end    = pd.to_datetime(PRE_END_TIME)\n",
    "post_start = pd.to_datetime(POST_START_TIME)\n",
    "post_end   = pd.to_datetime(POST_END_TIME)\n",
    "\n",
    "def slice_window(df, t0, t1):\n",
    "    m = (df[\"time\"] >= t0) & (df[\"time\"] <= t1)  # inclusive bounds [start, end]\n",
    "    return df.loc[m].copy()\n",
    "\n",
    "bia_pre  = slice_window(data_bia_analysis, pre_start, pre_end)\n",
    "bia_post = slice_window(data_bia_analysis, post_start, post_end)\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# POST_1 / POST_2 (hard segmentation, variance change-point MLE)\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "# Work on the POST PhA series\n",
    "s_post = pd.to_numeric(bia_post[\"PhA_48.8k_deg\"], errors=\"coerce\").to_numpy()\n",
    "finite_mask = np.isfinite(s_post)\n",
    "y = s_post[finite_mask]\n",
    "n = len(y)\n",
    "\n",
    "K_MIN = int(0.15 * n)  # avoid edges\n",
    "K_MAX = int(0.85 * n)\n",
    "\n",
    "# (Gaussian, different variances) MLE split: minimize n1*log(var1)+n2*log(var2)\n",
    "scores = []\n",
    "for k in range(K_MIN, K_MAX):\n",
    "    v1 = np.var(y[:k], ddof=1)\n",
    "    v2 = np.var(y[k:], ddof=1)\n",
    "    scores.append(len(y[:k]) * np.log(v1) + len(y[k:]) * np.log(v2))\n",
    "\n",
    "k_f = int(np.argmin(scores) + K_MIN)              # split index in finite-only y\n",
    "finite_idx = np.flatnonzero(finite_mask)          # mapping to dataframe rows\n",
    "k_df = int(finite_idx[k_f])                       # split index in bia_post rows\n",
    "split_time = bia_post[\"time\"].iloc[k_df]\n",
    "\n",
    "bia_post_1 = bia_post.iloc[:k_df].copy()\n",
    "bia_post_2 = bia_post.iloc[k_df:].copy()\n",
    "\n",
    "print(\"POST split_time =\", split_time)\n",
    "print(\"var POST_1 =\", float(pd.to_numeric(bia_post_1[\"PhA_48.8k_deg\"], errors=\"coerce\").var(ddof=1)))\n",
    "print(\"var POST_2 =\", float(pd.to_numeric(bia_post_2[\"PhA_48.8k_deg\"], errors=\"coerce\").var(ddof=1)))\n",
    "\n",
    "#==================================================================================\n",
    "#  PLOTS (just for verification)\n",
    "#==================================================================================\n",
    "\n",
    "def plot_window(df_win, title, y_col=\"PhA_48.8k_deg\", smooth_n=9):\n",
    "    if len(df_win) < 5:\n",
    "        print(\"Not enough points to plot:\", title)\n",
    "        return\n",
    "    d = df_win.copy()\n",
    "    d[\"time\"] = pd.to_datetime(d[\"time\"])\n",
    "    d = d.sort_values(\"time\")\n",
    "    y = pd.to_numeric(d[y_col], errors=\"coerce\")\n",
    "    y_sm = y.rolling(smooth_n, center=True, min_periods=1).mean()\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(d[\"time\"], y, alpha=0.7, label=\"raw\", color=\"#028A88\")  # teal\n",
    "    plt.plot(d[\"time\"], y_sm, label=f\"rolling mean (n={smooth_n})\", color=\"#F39EC7\")  # pink\n",
    "    plt.title(title, color=\"#000000\")\n",
    "    plt.xlabel(\"time\", color=\"#000000\")\n",
    "    plt.ylabel(y_col, color=\"#000000\")\n",
    "    plt.gca().set_facecolor(\"#FDFDFD\")  # light background\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_window(bia_pre,  \"BIA PRE (PhA_48.8k_deg)\")\n",
    "plot_window(bia_post, \"BIA POST (PhA_48.8k_deg)\")\n",
    "plot_window(bia_post_1, \"BIA POST_1 (PhA_48.8k_deg)\")\n",
    "plot_window(bia_post_2, \"BIA POST_2 (PhA_48.8k_deg)\")\n",
    "\n",
    "#==================================================================================\n",
    "# OUTPUT (variables to use)\n",
    "#==================================================================================\n",
    "\n",
    "pha_pre    = bia_pre[\"PhA_48.8k_deg\"].astype(float).dropna().to_numpy()\n",
    "pha_post   = bia_post[\"PhA_48.8k_deg\"].astype(float).dropna().to_numpy()\n",
    "pha_post_1 = bia_post_1[\"PhA_48.8k_deg\"].astype(float).dropna().to_numpy()\n",
    "pha_post_2 = bia_post_2[\"PhA_48.8k_deg\"].astype(float).dropna().to_numpy()\n",
    "\n",
    "t_pre  = (bia_pre[\"time\"]  - bia_pre[\"time\"].iloc[0]).dt.total_seconds().to_numpy()\n",
    "t_post = (bia_post[\"time\"] - bia_post[\"time\"].iloc[0]).dt.total_seconds().to_numpy()\n",
    "t_post_1 = (bia_post_1[\"time\"] - bia_post_1[\"time\"].iloc[0]).dt.total_seconds().to_numpy()\n",
    "t_post_2 = (bia_post_2[\"time\"] - bia_post_2[\"time\"].iloc[0]).dt.total_seconds().to_numpy()\n",
    "\n",
    "dt_pre_med  = float(bia_pre[\"time\"].diff().dt.total_seconds().median())\n",
    "dt_post_med = float(bia_post[\"time\"].diff().dt.total_seconds().median())\n",
    "dt_post1_med = float(bia_post_1[\"time\"].diff().dt.total_seconds().median())\n",
    "dt_post2_med = float(bia_post_2[\"time\"].diff().dt.total_seconds().median())\n",
    "\n",
    "fs_pre_est   = 1.0 / dt_pre_med\n",
    "fs_post_est  = 1.0 / dt_post_med\n",
    "fs_post1_est = 1.0 / dt_post1_med\n",
    "fs_post2_est = 1.0 / dt_post2_med\n",
    "\n",
    "print(\"\\nVARIABLES À UTILISER\")\n",
    "print(\"- bia_pre      : DataFrame PRE\")\n",
    "print(\"- bia_post     : DataFrame POST (global)\")\n",
    "print(\"- bia_post_1   : DataFrame POST_1 (avant split)\")\n",
    "print(\"- bia_post_2   : DataFrame POST_2 (après split)\")\n",
    "print(\"- pha_pre      : numpy array, PhA_48.8k_deg sur PRE\")\n",
    "print(\"- pha_post     : numpy array, PhA_48.8k_deg sur POST global\")\n",
    "print(\"- pha_post_1   : numpy array, PhA_48.8k_deg sur POST_1\")\n",
    "print(\"- pha_post_2   : numpy array, PhA_48.8k_deg sur POST_2\")\n",
    "print(\"- t_pre        : numpy array, temps (s) relatif au début de PRE\")\n",
    "print(\"- t_post       : numpy array, temps (s) relatif au début de POST (global)\")\n",
    "print(\"- t_post_1     : numpy array, temps (s) relatif au début de POST_1\")\n",
    "print(\"- t_post_2     : numpy array, temps (s) relatif au début de POST_2\")\n",
    "print(\"- fs_pre_est    : float, fs approx PRE (= 1 / dt_médian)\")\n",
    "print(\"- fs_post_est   : float, fs approx POST global\")\n",
    "print(\"- fs_post1_est  : float, fs approx POST_1\")\n",
    "print(\"- fs_post2_est  : float, fs approx POST_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2883670",
   "metadata": {},
   "source": [
    "# 1. STOCHASTIC PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e82cd",
   "metadata": {},
   "source": [
    "## 1.1 FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e0cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================\n",
    "# Global config \n",
    "#==========================\n",
    "\n",
    "Y_COL = \"PhA_48.8k_deg\"\n",
    "\n",
    "# We regularize the time grid because ACF/AR assume equally spaced samples.\n",
    "# 500 ms is close to the observed median dt (~0.513 s) in your windows.\n",
    "RESAMPLE_RULE = \"500ms\"\n",
    "\n",
    "# Lags used for ACF/PACF plots:\n",
    "# 40 lags at 0.5 s -> ~20 s horizon (enough to see medium-term dependence).\n",
    "PLOT_LAGS = 40\n",
    "\n",
    "# Residual whiteness test horizon:\n",
    "# 20 lags at 0.5 s -> ~10 s horizon (short-to-medium memory check).\n",
    "LB_LAG = 20\n",
    "\n",
    "# Candidate AR orders upper bound (clipped further by n//10 below).\n",
    "P_MAX_CAP = 20\n",
    "\n",
    "# Significance level for residual whiteness decision.\n",
    "ALPHA = 0.05\n",
    "\n",
    "\n",
    "\n",
    "# Optional: keep the notebook clean (does not change results)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "def prep_series(df_win, y_col=Y_COL, resample_rule=RESAMPLE_RULE):\n",
    "    \"\"\"\n",
    "    Prepare a clean 1D series for time-series tools (ACF/AR):\n",
    "      - parse/sort timestamps\n",
    "      - keep the signal as numeric\n",
    "      - regularize sampling (resample + interpolate) to enforce equal spacing\n",
    "\n",
    "    Returns:\n",
    "      y_out : pd.Series indexed by time\n",
    "      info  : small dict (n, dt medians, start/end) for traceability\n",
    "    \"\"\"\n",
    "    d = df_win[[\"time\", y_col]].copy()\n",
    "    d[\"time\"] = pd.to_datetime(d[\"time\"], errors=\"coerce\")\n",
    "    d = d.dropna(subset=[\"time\"]).sort_values(\"time\").set_index(\"time\")\n",
    "\n",
    "    y = pd.to_numeric(d[y_col], errors=\"coerce\")\n",
    "\n",
    "    # Raw dt (before resampling)\n",
    "    dt_raw = d.index.to_series().diff().dt.total_seconds().dropna()\n",
    "    dt_raw_med = float(dt_raw.median()) if len(dt_raw) else np.nan\n",
    "\n",
    "    if resample_rule is None:\n",
    "        # If raw dt is already regular enough, keep raw sampling\n",
    "        y_out = y.dropna()\n",
    "        dt_out = y_out.index.to_series().diff().dt.total_seconds().dropna()\n",
    "    else:\n",
    "        # Regular grid -> helps make ACF/AR assumptions explicit\n",
    "        y_out = y.resample(resample_rule).mean().interpolate(\"time\")\n",
    "        dt_out = y_out.index.to_series().diff().dt.total_seconds().dropna()\n",
    "\n",
    "    info = {\n",
    "        \"start\": d.index.min(),\n",
    "        \"end\": d.index.max(),\n",
    "        \"n_raw\": int(y.notna().sum()),\n",
    "        \"n_out\": int(y_out.notna().sum()),\n",
    "        \"dt_raw_med_s\": dt_raw_med,\n",
    "        \"dt_out_med_s\": float(dt_out.median()) if len(dt_out) else np.nan,\n",
    "        \"resample_rule\": resample_rule,\n",
    "    }\n",
    "    return y_out, info\n",
    "\n",
    "def plot_series_acf_pacf(y, name, plot_lags=PLOT_LAGS):\n",
    "    # 1) Series\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(y.index, y.values, color=\"#028A88\")  # teal\n",
    "    plt.title(f\"{name} | {Y_COL} (prepared)\")\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(Y_COL)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Compute ACF / PACF explicitly\n",
    "    acf_vals = acf(y.dropna(), nlags=plot_lags, fft=True)\n",
    "    pacf_vals = pacf(y.dropna(), nlags=plot_lags, method=\"ywm\")\n",
    "\n",
    "    lags = np.arange(1, plot_lags + 1)\n",
    "    stem_color = \"#028A88\"  # teal\n",
    "\n",
    "    # 2) ACF (exclude lag 0)\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    markerline, stemlines, baseline = plt.stem(lags, acf_vals[1:], basefmt=\" \")\n",
    "    plt.setp(markerline, color=stem_color)\n",
    "    plt.setp(stemlines, color=stem_color)\n",
    "    plt.axhline(0, color=\"k\", linewidth=0.8)\n",
    "    plt.axhspan(\n",
    "        -1.96 / np.sqrt(len(y)),\n",
    "         1.96 / np.sqrt(len(y)),\n",
    "        alpha=0.2, color = \"#F39EC7\"\n",
    "    )\n",
    "    plt.title(f\"{name} | ACF (lags 1–{plot_lags})\")\n",
    "    plt.xlabel(\"lag\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3) PACF (exclude lag 0)\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    markerline, stemlines, baseline = plt.stem(lags, pacf_vals[1:], basefmt=\" \")\n",
    "    plt.setp(markerline, color=stem_color)\n",
    "    plt.setp(stemlines, color=stem_color)\n",
    "    plt.axhline(0, color=\"k\", linewidth=0.8)\n",
    "    plt.axhspan(\n",
    "        -1.96 / np.sqrt(len(y)),\n",
    "         1.96 / np.sqrt(len(y)),\n",
    "        alpha=0.2, color = \"#F39EC7\"\n",
    "    )\n",
    "    plt.title(f\"{name} | PACF (lags 1–{plot_lags})\")\n",
    "    plt.xlabel(\"lag\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def stationarity_tests(y):\n",
    "    \"\"\"\n",
    "    ADF Test:\n",
    "      - ADF: H0 = unit root (non-stationary). Small p -> reject non-stationarity.\n",
    "    \"\"\"\n",
    "    yv = pd.Series(y).dropna().to_numpy()\n",
    "\n",
    "    # ADF (autolag chooses lag length via AIC internally)\n",
    "    adf_stat, adf_p, _, _, adf_crit, _ = adfuller(yv, autolag=\"AIC\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"adf_stat\": float(adf_stat),\n",
    "        \"adf_p\": float(adf_p),\n",
    "        \"adf_crit\": adf_crit,\n",
    "    }\n",
    "\n",
    "def need_diff_by_adf(adf_p, alpha=ALPHA):\n",
    "    # if we FAIL to reject unit root -> difference\n",
    "    return bool(adf_p > alpha)\n",
    "\n",
    "def print_stationarity(st, name):\n",
    "    print(f\"\\n{name} | stationarity tests\")\n",
    "    print(f\"ADF : stat={st['adf_stat']:.3f} | p={st['adf_p']:.4f}\")\n",
    "    \n",
    "\n",
    "def ar_grid_search(y_fit, p_max_cap=P_MAX_CAP):\n",
    "    \"\"\"\n",
    "    Fit AR(p) for p=1..p_max and report best AIC and best BIC\n",
    "    cap p to avoid overfitting on short windows\n",
    "    \"\"\"\n",
    "    yv = pd.Series(y_fit).dropna()\n",
    "    n = len(yv)\n",
    "    p_max = max(1, min(p_max_cap, n // 10))  # conservative rule\n",
    "\n",
    "    best_aic = None\n",
    "    best_bic = None\n",
    "\n",
    "    for p in range(1, p_max + 1):\n",
    "        try:\n",
    "            m = AutoReg(yv, lags=p, old_names=False).fit()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if (best_aic is None) or (m.aic < best_aic.aic):\n",
    "            best_aic = m\n",
    "        if (best_bic is None) or (m.bic < best_bic.bic):\n",
    "            best_bic = m\n",
    "\n",
    "    if best_aic is None or best_bic is None:\n",
    "        raise RuntimeError(\"AR grid search failed (try smaller p_max_cap).\")\n",
    "\n",
    "    return {\n",
    "        \"p_max_used\": p_max,\n",
    "        \"model_aic\": best_aic,\n",
    "        \"p_aic\": int(best_aic.model._maxlag),\n",
    "        \"model_bic\": best_bic,\n",
    "        \"p_bic\": int(best_bic.model._maxlag),\n",
    "    }\n",
    "\n",
    "\n",
    "def choose_p_by_whiteness(y_fit, p_max_cap=P_MAX_CAP, lb_lag=LB_LAG, alpha=ALPHA):\n",
    "    \"\"\"\n",
    "    choose the smallest p such that Ljung-Box p-value > alpha.\n",
    "    If none passes, fall back to best-BIC (+report the limitation).\n",
    "    \"\"\"\n",
    "    yv = pd.Series(y_fit).dropna()\n",
    "    n = len(yv)\n",
    "    p_max = max(1, min(p_max_cap, n // 10))\n",
    "\n",
    "    rows = []\n",
    "    models = {}\n",
    "\n",
    "    for p in range(1, p_max + 1):\n",
    "        try:\n",
    "            m = AutoReg(yv, lags=p, old_names=False).fit()\n",
    "            resid = pd.Series(m.resid).dropna()\n",
    "\n",
    "            lb = acorr_ljungbox(resid, lags=[lb_lag], return_df=True)\n",
    "            lb_p = float(lb[\"lb_pvalue\"].iloc[0])\n",
    "\n",
    "            rows.append((p, float(m.aic), float(m.bic), lb_p))\n",
    "            models[p] = m\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    scan = pd.DataFrame(rows, columns=[\"p\", \"AIC\", \"BIC\", f\"LB_p_lag{lb_lag}\"]).sort_values(\"p\")\n",
    "\n",
    "    ok = scan[scan[f\"LB_p_lag{lb_lag}\"] > alpha]\n",
    "    if len(ok):\n",
    "        p_star = int(ok.iloc[0][\"p\"])\n",
    "        return {\"p\": p_star, \"model\": models[p_star], \"passed\": True, \"p_max_used\": p_max}\n",
    "    else:\n",
    "        # fallback: best BIC among scanned models\n",
    "        p_bic = int(scan.loc[scan[\"BIC\"].idxmin(), \"p\"])\n",
    "        return {\"p\": p_bic, \"model\": models[p_bic], \"passed\": False, \"p_max_used\": p_max}\n",
    "\n",
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def residual_diagnostics(model, name, lb_lag=LB_LAG, plot_lags=PLOT_LAGS):\n",
    "    \"\"\"\n",
    "    Minimal diagnostics:\n",
    "      - Residual ACF (visual) excluding lag 0 (trivially = 1)\n",
    "      - Ljung-Box p-value at fixed lag (statistical)\n",
    "    \"\"\"\n",
    "    resid = pd.Series(model.resid).dropna()\n",
    "\n",
    "    # Compute residual ACF explicitly to drop lag 0 from the plot.\n",
    "    acf_vals = acf(resid.to_numpy(), nlags=plot_lags, fft=True)\n",
    "    lags = np.arange(1, plot_lags + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    markerline, stemlines, baseline = plt.stem(lags, acf_vals[1:], basefmt=\" \")\n",
    "    plt.setp(markerline, color=\"#028A88\")\n",
    "    plt.setp(stemlines, color=\"#028A88\")\n",
    "    plt.axhline(0, color=\"k\", linewidth=0.8)\n",
    "\n",
    "    # simple approx CI band (same idea as statsmodels' default)\n",
    "    ci = 1.96 / np.sqrt(len(resid))\n",
    "    plt.axhspan(-ci, ci, alpha=0.2, color = \"#F39EC7\")\n",
    "\n",
    "    plt.title(f\"{name} | Residual ACF (lags 1–{plot_lags})\")\n",
    "    plt.xlabel(\"lag\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Ljung-Box whiteness test at a fixed horizon\n",
    "    lb = acorr_ljungbox(resid, lags=[lb_lag], return_df=True)\n",
    "    lb_p = float(lb[\"lb_pvalue\"].iloc[0])\n",
    "\n",
    "    return {\n",
    "        \"lb_p\": lb_p,\n",
    "        \"resid_var\": float(np.var(resid, ddof=1)),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def run_ar_block(y_fit, name, diff_used, force_diff=False):\n",
    "    \"\"\"\n",
    "    One window:\n",
    "      - report best AIC and best BIC AR(p)\n",
    "      - pick final p by whiteness \n",
    "      - report a small set of parameters\n",
    "      - residual diagnostics\n",
    "    \"\"\"\n",
    "    # enforce differencing here (if requested)\n",
    "    if force_diff:\n",
    "        y_fit = pd.Series(y_fit).dropna().diff().dropna()\n",
    "        diff_used = True\n",
    "\n",
    "    print(f\"\\n {name} | AR modeling\")\n",
    "    print(f\"Series used: diff1={diff_used} | n={len(y_fit)}\")\n",
    "\n",
    "    grid = ar_grid_search(y_fit)\n",
    "    print(f\"AR grid search: p_max_used={grid['p_max_used']}\")\n",
    "    print(f\"Best by AIC: p={grid['p_aic']} | AIC={grid['model_aic'].aic:.3f} | BIC={grid['model_aic'].bic:.3f}\")\n",
    "    print(f\"Best by BIC: p={grid['p_bic']} | AIC={grid['model_bic'].aic:.3f} | BIC={grid['model_bic'].bic:.3f}\")\n",
    "\n",
    "    chosen = choose_p_by_whiteness(y_fit)\n",
    "    final_p = chosen[\"p\"]\n",
    "    final_model = chosen[\"model\"]\n",
    "\n",
    "    if chosen[\"passed\"]:\n",
    "        print(f\"Final AR(p) chosen by whiteness: p={final_p}\")\n",
    "    else:\n",
    "        print(f\"Final AR(p): no p<=p_max achieved Ljung-Box p>{ALPHA} at lag={LB_LAG}.\")\n",
    "        print(f\"Using best-BIC fallback: p={final_p}\")\n",
    "        print(\"Note: remaining autocorrelation suggests ARMA may be more appropriate.\")\n",
    "\n",
    "    print(\"Params (head):\")\n",
    "    print(final_model.params.head(6))\n",
    "\n",
    "    diag = residual_diagnostics(final_model, name)\n",
    "\n",
    "    print(f\"Ljung-Box (lag={LB_LAG}): p={diag['lb_p']:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"diff1\": diff_used,\n",
    "        \"p_aic\": grid[\"p_aic\"],\n",
    "        \"p_bic\": grid[\"p_bic\"],\n",
    "        \"p_final\": int(final_p),\n",
    "        \"aic_final\": float(final_model.aic),\n",
    "        \"bic_final\": float(final_model.bic),\n",
    "        \"lb_p\": diag[\"lb_p\"],\n",
    "        \"resid_var\": diag[\"resid_var\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af34c3",
   "metadata": {},
   "source": [
    "## 1.2 PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. STOCHASTIC PROCESS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Prepare both windows (this is the input for all later steps)\n",
    "y_pre, info_pre = prep_series(bia_pre)\n",
    "y_post, info_post = prep_series(bia_post)\n",
    "y_post1, info_post1 = prep_series(bia_post_1)  # alternative POST window for sensitivity check\n",
    "y_post2, info_post2 = prep_series(bia_post_2)  # alternative POST window for sensitivity check\n",
    "\n",
    "print(\"PRE :\", f\"n_raw={info_pre['n_raw']}, n_out={info_pre['n_out']}, \"\n",
    "             f\"dt_raw_med={info_pre['dt_raw_med_s']:.3f}s, dt_out_med={info_pre['dt_out_med_s']:.3f}s, \"\n",
    "             f\"range={info_pre['start']} -> {info_pre['end']}, resample={info_pre['resample_rule']}\")\n",
    "\n",
    "print(\"POST:\", f\"n_raw={info_post['n_raw']}, n_out={info_post['n_out']}, \"\n",
    "             f\"dt_raw_med={info_post['dt_raw_med_s']:.3f}s, dt_out_med={info_post['dt_out_med_s']:.3f}s, \"\n",
    "             f\"range={info_post['start']} -> {info_post['end']}, resample={info_post['resample_rule']}\")\n",
    "\n",
    "# Plots for narrative: first PRE, then POST\n",
    "plot_series_acf_pacf(y_pre, \"PRE\")\n",
    "plot_series_acf_pacf(y_post, \"POST\")\n",
    "plot_series_acf_pacf(y_post1, \"POST1\")\n",
    "plot_series_acf_pacf(y_post2, \"POST2\")\n",
    "\n",
    "# Run tests on prepared series \n",
    "st_pre = stationarity_tests(y_pre)\n",
    "st_post = stationarity_tests(y_post)\n",
    "st_post1 = stationarity_tests(y_post1)\n",
    "st_post2 = stationarity_tests(y_post2)\n",
    "\n",
    "print_stationarity(st_pre, \"PRE\")\n",
    "print_stationarity(st_post, \"POST\")\n",
    "print_stationarity(st_post1, \"POST1\")\n",
    "print_stationarity(st_post2, \"POST2\")\n",
    "\n",
    "\n",
    "# Decision step for AR modeling \n",
    "# AR models assume stationarity; if ADF does not reject non-stationarity (p>0.05),\n",
    "# we use first difference (diff1) as a simple stationarization step.\n",
    "y_pre_fit = y_pre.dropna()\n",
    "y_post_fit = y_post.dropna()\n",
    "y_post1_fit = y_post1.dropna()\n",
    "y_post2_fit = y_post2.dropna()\n",
    "\n",
    "diff_pre = False\n",
    "diff_post = False\n",
    "diff_post1 = False\n",
    "diff_post2 = False\n",
    "\n",
    "if st_pre[\"adf_p\"] > 0.05:\n",
    "    y_pre_fit = y_pre_fit.diff().dropna()\n",
    "    diff_pre = True\n",
    "\n",
    "if st_post[\"adf_p\"] > 0.05:\n",
    "    y_post_fit = y_post_fit.diff().dropna()\n",
    "    diff_post = True\n",
    "\n",
    "if st_post1[\"adf_p\"] > 0.05:\n",
    "    y_post1_fit = y_post1_fit.diff().dropna()\n",
    "    diff_post1 = True\n",
    "\n",
    "if st_post2[\"adf_p\"] > 0.05:\n",
    "    y_post2_fit = y_post2_fit.diff().dropna()\n",
    "    diff_post2 = True\n",
    "\n",
    "\n",
    "print(\"\\nModeling series (after stationarity step)\")\n",
    "print(f\"PRE : diff1={diff_pre} | n={len(y_pre_fit)}\")\n",
    "print(f\"POST: diff1={diff_post} | n={len(y_post_fit)}\")\n",
    "print(f\"POST1: diff1={diff_post1} | n={len(y_post1_fit)}\")\n",
    "print(f\"POST2: diff1={diff_post2} | n={len(y_post2_fit)}\")\n",
    "\n",
    "# Run both windows\n",
    "out_pre = run_ar_block(y_pre_fit, \"PRE\", diff_pre)\n",
    "out_post = run_ar_block(y_post_fit, \"POST\", diff_post)\n",
    "out_post1 = run_ar_block(y_post1_fit, \"POST1\", diff_post1)\n",
    "out_post2 = run_ar_block(y_post2_fit, \"POST2\", diff_post2)\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"Window\": \"PRE\",\n",
    "        \"n_prep\": info_pre[\"n_out\"],\n",
    "        \"dt_out_med_s\": info_pre[\"dt_out_med_s\"],\n",
    "        \"ADF_p\": st_pre[\"adf_p\"],\n",
    "        \"diff1\": diff_pre,\n",
    "        \"p_AIC\": out_pre[\"p_aic\"],\n",
    "        \"p_BIC\": out_pre[\"p_bic\"],\n",
    "        \"p_final\": out_pre[\"p_final\"],\n",
    "        \"AIC_final\": out_pre[\"aic_final\"],\n",
    "        \"BIC_final\": out_pre[\"bic_final\"],\n",
    "        \"LB_p_lag20\": out_pre[\"lb_p\"],\n",
    "        \"resid_var\": out_pre[\"resid_var\"],\n",
    "    },\n",
    "    {\n",
    "        \"Window\": \"POST\",\n",
    "        \"n_prep\": info_post[\"n_out\"],\n",
    "        \"dt_out_med_s\": info_post[\"dt_out_med_s\"],\n",
    "        \"ADF_p\": st_post[\"adf_p\"],\n",
    "        \"diff1\": diff_post,\n",
    "        \"p_AIC\": out_post[\"p_aic\"],\n",
    "        \"p_BIC\": out_post[\"p_bic\"],\n",
    "        \"p_final\": out_post[\"p_final\"],\n",
    "        \"AIC_final\": out_post[\"aic_final\"],\n",
    "        \"BIC_final\": out_post[\"bic_final\"],\n",
    "        \"LB_p_lag20\": out_post[\"lb_p\"],\n",
    "        \"resid_var\": out_post[\"resid_var\"],\n",
    "    },\n",
    "    {\n",
    "        \"Window\": \"POST1\",\n",
    "        \"n_prep\": info_post1[\"n_out\"],\n",
    "        \"dt_out_med_s\": info_post1[\"dt_out_med_s\"],\n",
    "        \"ADF_p\": st_post1[\"adf_p\"],\n",
    "        \"diff1\": diff_post1,\n",
    "        \"p_AIC\": out_post1[\"p_aic\"],\n",
    "        \"p_BIC\": out_post1[\"p_bic\"],\n",
    "        \"p_final\": out_post1[\"p_final\"],\n",
    "        \"AIC_final\": out_post1[\"aic_final\"],\n",
    "        \"BIC_final\": out_post1[\"bic_final\"],\n",
    "        \"LB_p_lag20\": out_post1[\"lb_p\"],\n",
    "        \"resid_var\": out_post1[\"resid_var\"],\n",
    "    },\n",
    "    {\n",
    "        \"Window\": \"POST2\",\n",
    "        \"n_prep\": info_post2[\"n_out\"],\n",
    "        \"dt_out_med_s\": info_post2[\"dt_out_med_s\"],\n",
    "        \"ADF_p\": st_post2[\"adf_p\"],\n",
    "        \"diff1\": diff_post2,\n",
    "        \"p_AIC\": out_post2[\"p_aic\"],\n",
    "        \"p_BIC\": out_post2[\"p_bic\"],\n",
    "        \"p_final\": out_post2[\"p_final\"],\n",
    "        \"AIC_final\": out_post2[\"aic_final\"],\n",
    "        \"BIC_final\": out_post2[\"bic_final\"],\n",
    "        \"LB_p_lag20\": out_post2[\"lb_p\"],\n",
    "        \"resid_var\": out_post2[\"resid_var\"],\n",
    "    },\n",
    "])\n",
    "\n",
    "# Compact display (rounded)\n",
    "summary_print = summary.copy()\n",
    "for c in [\"dt_out_med_s\",\"ADF_p\",\"AIC_final\",\"BIC_final\",\"LB_p_lag20\",\"resid_var\"]:\n",
    "    summary_print[c] = summary_print[c].astype(float).round(6)\n",
    "\n",
    "print(\"\\nPRE vs POST vs POST1 vs POST2 (summary)\")\n",
    "print(summary_print.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604937ed",
   "metadata": {},
   "source": [
    "# 2. SPECTRAL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f0511",
   "metadata": {},
   "source": [
    "## 2.1 FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26cfc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================\n",
    "#SPECTRAL ANALYSE\n",
    "#==============================================================\n",
    "\n",
    "class SpectralAnalysis:\n",
    "\n",
    "#-------------------------\n",
    "# Filter function \n",
    "#-------------------------\n",
    "    def butter_lowpass_filter(data, cutoff, fs, order=2):\n",
    "        nyq = 0.5 * fs\n",
    "        normal_cutoff = cutoff / nyq\n",
    "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "        y = filtfilt(b, a, data)\n",
    "        return y\n",
    "    \n",
    "    def remove_mean_and_detrend(signal):\n",
    "        x = signal - np.mean(signal)\n",
    "        x = detrend(x)\n",
    "        return x\n",
    "\n",
    "    def cosine_taper(signal, p=0.1):\n",
    "        \"\"\"\n",
    "        Applique un cosin tapering (fenêtre de Tukey) au signal.\n",
    "        p : proportion de la fenêtre cosinus (0 = rectangulaire, 1 = Hann)\n",
    "        \"\"\"\n",
    "        from scipy.signal.windows import tukey\n",
    "        w = tukey(len(signal), alpha=p)\n",
    "        return signal * w\n",
    "\n",
    "#------------------------\n",
    "# FFT function\n",
    "#------------------------\n",
    "    def compute_fft_power_spectrum(signal, fs, window='hann'):\n",
    "        n = len(signal)\n",
    "        fft_vals = np.fft.rfft(signal)\n",
    "        freqs = np.fft.rfftfreq(n, d=1.0/fs)\n",
    "        power = np.abs(fft_vals)**2 / n\n",
    "        return freqs, power\n",
    "    \n",
    "#------------------------\n",
    "# Plotting function\n",
    "#------------------------\n",
    "    def plot_compare_power_spectra(list_freqs, list_power, labels, title, xlabel=\"Frequency (Hz)\", ylabel=\"Power\", logy=True):\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        colors = [\"#028A88\", \"#F39EC7\", \"#d62728\", \"#1f77b4\", \"#FFA500\"]\n",
    "        for idx, (freqs, power, label) in enumerate(zip(list_freqs, list_power, labels)):\n",
    "            color = colors[idx % len(colors)]\n",
    "            if logy:\n",
    "                plt.semilogy(freqs, power, label=label, color=color)\n",
    "            else:\n",
    "                plt.plot(freqs, power, label=label, color=color)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_raw_and_filtered_panels(\n",
    "        data_list,\n",
    "        titles=None,\n",
    "        xlabels=None,\n",
    "        ylabels=None,\n",
    "        suptitle=None,\n",
    "        figsize=(16, 5)\n",
    "    ):\n",
    "        n = len(data_list)\n",
    "        fig, axes = plt.subplots(1, n, figsize=figsize, squeeze=False)\n",
    "        if suptitle:\n",
    "            fig.suptitle(suptitle, fontsize=16, fontweight='bold')\n",
    "        for i, data in enumerate(data_list):\n",
    "            ax = axes[0, i]\n",
    "            ax.plot(data['time'], data['raw'], alpha=0.5, linewidth=0.5, label='Raw', color=\"#028A88\")\n",
    "            ax.plot(data['time'], data['filtered'], linewidth=2, label='Filtered', color=\"#F39EC7\")\n",
    "            if titles and i < len(titles):\n",
    "                ax.set_title(titles[i])\n",
    "            if xlabels and i < len(xlabels):\n",
    "                ax.set_xlabel(xlabels[i])\n",
    "            else:\n",
    "                ax.set_xlabel('Time (s)')\n",
    "            if ylabels and i < len(ylabels):\n",
    "                ax.set_ylabel(ylabels[i])\n",
    "            else:\n",
    "                ax.set_ylabel('Phase Angle (°)')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_welch_panels(\n",
    "        list_freqs,\n",
    "        list_psd,\n",
    "        titles=None,\n",
    "        xlabels=None,\n",
    "        ylabels=None,\n",
    "        suptitle=None,\n",
    "        figsize=(14, 4)\n",
    "    ):\n",
    "        n = len(list_freqs)\n",
    "        colors = [\"#028A88\", \"#F39EC7\", \"#d62728\", \"#1f77b4\", \"#FFA500\"]\n",
    "        fig, axes = plt.subplots(1, n, figsize=figsize, squeeze=False)\n",
    "        if suptitle:\n",
    "            fig.suptitle(suptitle, fontsize=16, fontweight='bold')\n",
    "        for i in range(n):\n",
    "            color = colors[i % len(colors)]\n",
    "            ax = axes[0, i]\n",
    "            ax.semilogy(list_freqs[i], list_psd[i], color=color)\n",
    "            if titles and i < len(titles):\n",
    "                ax.set_title(titles[i])\n",
    "            if xlabels and i < len(xlabels):\n",
    "                ax.set_xlabel(xlabels[i])\n",
    "            else:\n",
    "                ax.set_xlabel('Frequency (Hz)')\n",
    "            if ylabels and i < len(ylabels):\n",
    "                ax.set_ylabel(ylabels[i])\n",
    "            else:\n",
    "                ax.set_ylabel('PSD')\n",
    "            ax.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_log_psd_with_fit(freqs, psd, label=None,fmin=None,fmax=None, title=\"log(PSD) vs log(f) avec fit\", xlabel=\"log10(Frequency [Hz])\", ylabel=\"log10(PSD)\"):\n",
    "        # Filtrer les fréquences et PSD > 0 pour éviter log(0)\n",
    "        mask = (freqs > 0) & (psd > 0)\n",
    "        if fmin is not None:\n",
    "            mask &= freqs >= fmin\n",
    "        if fmax is not None:\n",
    "            mask &= freqs <= fmax\n",
    "\n",
    "        log_freqs = np.log10(freqs[mask])\n",
    "        log_psd = np.log10(psd[mask])\n",
    "        if len(log_freqs) < 3:\n",
    "            print(\"Pas assez de points pour un fit fiable.\")\n",
    "            return None\n",
    "    \n",
    "        # Régression linéaire\n",
    "        coeffs = np.polyfit(log_freqs, log_psd, 1)\n",
    "        fit_line = np.polyval(coeffs, log_freqs)\n",
    "        coeffs = np.polyfit(log_freqs, log_psd, 1)\n",
    "        slope, intercept = coeffs\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(log_freqs, log_psd, label=label)\n",
    "        plt.plot(log_freqs, fit_line, 'r--', label=f'Fit: y={coeffs[0]:.2f}x+{coeffs[1]:.2f}')\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\", ls=\"--\")\n",
    "        plt.show()\n",
    "        return slope, intercept\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136b9b5",
   "metadata": {},
   "source": [
    "## 2.2 DEFINING BUTTERWORTH ORDER / CUTOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# TEST BUTTERWORTH DESIGN\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "Fs = 2\n",
    "Nyq = Fs / 2\n",
    "\n",
    "fp = 0.05\n",
    "fs = 0.2\n",
    "\n",
    "wp = fp / Nyq\n",
    "ws = fs / Nyq\n",
    "\n",
    "Ap = 1     # dB\n",
    "As = 40    # dB\n",
    "\n",
    "n, Wn = buttord(wp, ws, Ap, As)\n",
    "\n",
    "\n",
    "print(\"Ordre :\", n)\n",
    "print(\"Cutoff :\", Wn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16903fdd",
   "metadata": {},
   "source": [
    "## 2.3 PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  2. SPECTRAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Create stationary signals and time vectors\n",
    "#--------------------------------------------------\n",
    "\n",
    "#-------- Signal preprocessing: detrend ---------\n",
    "# Detrend signals\n",
    "pha_post_1_detrend = remove_mean_and_detrend(np.asarray(pha_post_1))\n",
    "pha_pre_detrend = remove_mean_and_detrend(np.asarray(pha_pre))\n",
    "pha_post_detrend = remove_mean_and_detrend(np.asarray(pha_post))\n",
    "\n",
    "# time vectors for plotting (match lengths after detrending)\n",
    "t_post_1_detrend_plot = t_post_1[:len(pha_post_1_detrend)]\n",
    "pha_post_1_detrend_plot = pha_post_1_detrend[:len(t_post_1_detrend_plot)]\n",
    "pha_post_1_filtered_plot = pha_post_1_filtered[:len(t_post_1_detrend_plot)]\n",
    "\n",
    "t_pre_detrend_plot = t_pre[:len(pha_pre_detrend)]\n",
    "pha_pre_detrend_plot = pha_pre_detrend[:len(t_pre_detrend_plot)]\n",
    "pha_pre_filtered_plot = pha_pre_filtered[:len(t_pre_detrend_plot)]\n",
    "\n",
    "t_post_detrend_plot = t_post[:len(pha_post_detrend)]\n",
    "pha_post_detrend_plot = pha_post_detrend[:len(t_post_detrend_plot)]\n",
    "pha_post_filtered_plot = pha_post_filtered[:len(t_post_detrend_plot)]\n",
    "\n",
    "#-------- Signal preprocessing: tapering ---------\n",
    "# Apply cosine tapering\n",
    "pha_pre_detrend_tapered = cosine_taper(pha_pre_detrend, p=0.1)\n",
    "pha_post_1_detrend_tapered = cosine_taper(pha_post_1_detrend, p=0.1)\n",
    "pha_post_detrend_tapered = cosine_taper(pha_post_detrend, p=0.1)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 1. Raw and Butterworth Filtered Signals\n",
    "#-------------------------------------------------\n",
    "cutoff_freq = 0.06 # Hz (determined from Butterworth design test)\n",
    "#----------- Def Fc and filter order -------------------\n",
    "# Apply Butterworth lowpass filter\n",
    "\n",
    "pha_pre_filtered = SpectralAnalysis.butter_lowpass_filter(pha_pre_detrend_tapered, cutoff=cutoff_freq, fs=fs_pre_est, order=2)\n",
    "pha_post_filtered = SpectralAnalysis.butter_lowpass_filter(pha_post_detrend_tapered, cutoff=cutoff_freq, fs=fs_post_est, order=2)\n",
    "pha_post_1_filtered = SpectralAnalysis.butter_lowpass_filter(pha_post_1_detrend_tapered, cutoff=cutoff_freq, fs=fs_post1_est, order=2)\n",
    "pha_post_2_filtered = SpectralAnalysis.butter_lowpass_filter(pha_post_2, cutoff=cutoff_freq, fs=fs_post2_est, order=2)\n",
    "\n",
    "#----------- Plot raw and filtered signals -------------\n",
    "# POST\n",
    "SpectralAnalysis.plot_raw_and_filtered_panels(\n",
    "    [{'time': t_post_detrend_plot, 'raw': pha_post_detrend_plot, 'filtered': pha_post_filtered_plot}],\n",
    "    titles=['BIA Phase Angle - POST Fatigue']\n",
    ")\n",
    "\n",
    "# PRE vs POST1 vs POST2\n",
    "SpectralAnalysis.plot_raw_and_filtered_panels(\n",
    "    [\n",
    "        {'time': t_pre_detrend_plot, 'raw': pha_pre_detrend_plot, 'filtered': pha_pre_filtered_plot},\n",
    "        {'time': t_post_1_detrend_plot, 'raw': pha_post_1_detrend_plot, 'filtered': pha_post_1_filtered_plot},\n",
    "        {'time': t_post_2, 'raw': pha_post_2, 'filtered': pha_post_2_filtered},\n",
    "    ],\n",
    "    titles=['PRE', 'POST_1', 'POST_2'],\n",
    "    suptitle='BIA Phase Angle (Raw & Filtered) — All Windows'\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 2. FFT Power Spectrum Comparison\n",
    "#-------------------------------------------------\n",
    "\n",
    "# ----------- Def the FFT signal --------------\n",
    "freqs_fft_pre, power_fft_pre = SpectralAnalysis.compute_fft_power_spectrum(pha_pre_detrend_tapered, fs_pre_est)\n",
    "freqs_fft_post, power_fft_post = SpectralAnalysis.compute_fft_power_spectrum(pha_post_detrend_tapered, fs_post_est)\n",
    "freqs_fft_post1, power_fft_post1 = SpectralAnalysis.compute_fft_power_spectrum(pha_post_1_detrend_tapered, fs_post1_est)\n",
    "freqs_fft_post2, power_fft_post2 = SpectralAnalysis.compute_fft_power_spectrum(pha_post_2, fs_post2_est)\n",
    "\n",
    "# ----------- Plot comparisons --------------\n",
    "# PRE vs POST\n",
    "SpectralAnalysis.plot_compare_power_spectra(\n",
    "    [freqs_fft_pre, freqs_fft_post],\n",
    "    [power_fft_pre, power_fft_post],\n",
    "    [f'PRE-fatigue (fs={fs_pre_est:.2f} Hz)', f'POST-fatigue (fs={fs_post_est:.2f} Hz)'],\n",
    "    title='FFT Power Spectrum Comparison: PRE vs POST Muscle Fatigue',\n",
    "    ylabel='Power (°²)'\n",
    ")\n",
    "\n",
    "# PRE vs POST1 vs POST2\n",
    "SpectralAnalysis.plot_compare_power_spectra(\n",
    "    [freqs_fft_pre, freqs_fft_post1, freqs_fft_post2],\n",
    "    [power_fft_pre, power_fft_post1, power_fft_post2],\n",
    "    [\n",
    "        f'PRE (fs={fs_pre_est:.2f} Hz)',\n",
    "        f'POST_1 (fs={fs_post1_est:.2f} Hz)',\n",
    "        f'POST_2 (fs={fs_post2_est:.2f} Hz)'\n",
    "    ],\n",
    "    title='FFT Power Spectrum Comparison: PRE vs POST_1 vs POST_2',\n",
    "    ylabel='Power (°²)'\n",
    ")\n",
    "#-------------------------------------------------\n",
    "# 3. Smoothed FFT Power Spectrum Comparison (Butterworth Filtered Signals)\n",
    "#-------------------------------------------------\n",
    "\n",
    "# ----------- Def the FFT filtered signal --------------\n",
    "freqs_fft_pre_filt, power_fft_pre_filt = SpectralAnalysis.compute_fft_power_spectrum(pha_pre_filtered, fs_pre_est)\n",
    "freqs_fft_post_filt, power_fft_post_filt = SpectralAnalysis.compute_fft_power_spectrum(pha_post_filtered, fs_post_est)\n",
    "\n",
    "# ----------- Plot comparisons --------------\n",
    "# PRE vs POST\n",
    "SpectralAnalysis.plot_compare_power_spectra(\n",
    "    [freqs_fft_pre_filt, freqs_fft_post_filt],\n",
    "    [power_fft_pre_filt, power_fft_post_filt],\n",
    "    [f'PRE Butterworth (fs={fs_pre_est:.2f} Hz)', f'POST Butterworth (fs={fs_post_est:.2f} Hz)'],\n",
    "    title='FFT Power Spectrum (Butterworth filtered): PRE vs POST',\n",
    "    ylabel='Power (°²)'\n",
    ")\n",
    "# PRE vs POST1 vs POST2\n",
    "freqs_fft_post1_filt, power_fft_post1_filt = SpectralAnalysis.compute_fft_power_spectrum(pha_post_1_filtered, fs_post1_est)\n",
    "freqs_fft_post2_filt, power_fft_post2_filt = SpectralAnalysis.compute_fft_power_spectrum(pha_post_2_filtered, fs_post2_est)\n",
    "SpectralAnalysis.plot_compare_power_spectra(\n",
    "    [freqs_fft_pre_filt, freqs_fft_post1_filt, freqs_fft_post2_filt],\n",
    "    [power_fft_pre_filt, power_fft_post1_filt, power_fft_post2_filt],\n",
    "    [\n",
    "        f'PRE Butterworth (fs={fs_pre_est:.2f} Hz)',\n",
    "        f'POST_1 Butterworth (fs={fs_post1_est:.2f} Hz)',\n",
    "        f'POST_2 Butterworth (fs={fs_post2_est:.2f} Hz)'\n",
    "    ],\n",
    "    title='FFT Power Spectrum (Butterworth filtered): PRE vs POST_1 vs POST_2',\n",
    "    ylabel='Power (°²)'\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 4. Welch Periodogram (Individual)\n",
    "#-------------------------------------------------\n",
    "\n",
    "# ----------- Def the Welch signal --------------\n",
    "freqs_pre, psd_pre = welch(pha_pre_detrend, fs=fs_pre_est, nperseg=len(pha_post_detrend), noverlap=0)\n",
    "freqs_post1, psd_post1 = welch(np.asarray(pha_post_1_detrend), fs=fs_post1_est, nperseg=len(pha_post_detrend), noverlap=0)\n",
    "freqs_post2, psd_post2 = welch(pha_post_2, fs=fs_post2_est, nperseg=len(pha_post_2_filtered), noverlap=0)\n",
    "freqs_post, psd_post = welch(pha_post_detrend, fs=fs_post_est, nperseg=len(pha_post_detrend), noverlap=0)\n",
    "\n",
    "# ----------- Plot comparisons --------------\n",
    "# PRE vs POST\n",
    "SpectralAnalysis.plot_compare_power_spectra(\n",
    "    [freqs_pre, freqs_post],\n",
    "    [psd_pre, psd_post],\n",
    "    [f'PRE (fs={fs_pre_est:.2f} Hz)', f'POST (fs={fs_post_est:.2f} Hz)'],\n",
    "    title='Welch Periodogram Comparison: PRE vs POST Muscle Fatigue',\n",
    "    ylabel='PSD'\n",
    ")\n",
    "\n",
    "# PRE vs POST1 vs POST2\n",
    "SpectralAnalysis.plot_compare_power_spectra(\n",
    "    [freqs_pre, freqs_post1, freqs_post2],\n",
    "    [psd_pre, psd_post1, psd_post2],\n",
    "    [\n",
    "        f'PRE (fs={fs_pre_est:.2f} Hz)',\n",
    "        f'POST_1 (fs={fs_post1_est:.2f} Hz)',\n",
    "        f'POST_2 (fs={fs_post2_est:.2f} Hz)'\n",
    "    ],\n",
    "    title='Welch Periodogram Comparison: PRE vs POST_1 vs POST_2',\n",
    "    ylabel='PSD'\n",
    ")\n",
    "\n",
    "SpectralAnalysis.plot_log_psd_with_fit(freqs_pre, psd_pre, fmin=0.03, fmax=0.3, label=\"PRE\")\n",
    "SpectralAnalysis.plot_log_psd_with_fit(freqs_post, psd_post, fmin=0.03, fmax=0.3, label=\"POST\")\n",
    "SpectralAnalysis.plot_log_psd_with_fit(freqs_post1, psd_post1, fmin=0.03, fmax=0.3, label=\"POST1\")\n",
    "SpectralAnalysis.plot_log_psd_with_fit(freqs_post2, psd_post2, fmin=0.03, fmax=0.3, label=\"POST2\")\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 5. Dominant Frequency\n",
    "#-------------------------------------------------\n",
    "\n",
    "#----------- Find dominant frequencies --------------\n",
    "# PRE\n",
    "idx_dom_pre = np.argmax(psd_pre)\n",
    "f_dom_pre = freqs_pre[idx_dom_pre]\n",
    "P_dom_pre = psd_pre[idx_dom_pre]\n",
    "# POST\n",
    "idx_dom_post = np.argmax(psd_post)\n",
    "f_dom_post = freqs_post[idx_dom_post]\n",
    "P_dom_post = psd_post[idx_dom_post]\n",
    "#POST1\n",
    "idx_dom_post1 = np.argmax(psd_post1)\n",
    "f_dom_post1 = freqs_post1[idx_dom_post1]\n",
    "P_dom_post1 = psd_post1[idx_dom_post1]\n",
    "#POST2\n",
    "idx_dom_post2 = np.argmax(psd_post2)\n",
    "f_dom_post2 = freqs_post2[idx_dom_post2]\n",
    "P_dom_post2 = psd_post2[idx_dom_post2]\n",
    "\n",
    "# ----------- Print dominant frequencies --------------\n",
    "print(f\"Dominant Frequency PRE: f={f_dom_pre:.3f} Hz | PSD={P_dom_pre:.3f} °²/Hz\")\n",
    "print(f\"Dominant Frequency POST: f={f_dom_post:.3f} Hz | PSD={P_dom_post:.3f} °²/Hz\")\n",
    "print(f\"Dominant Frequency POST1: f={f_dom_post1:.3f} Hz | PSD={P_dom_post1:.3f} °²/Hz\")\n",
    "print(f\"Dominant Frequency POST2: f={f_dom_post2:.3f} Hz | PSD={P_dom_post2:.3f} °²/Hz\")\n",
    "\n",
    "#----------- Find dominant frequencies (Butterworth filtered) --------------\n",
    "# PRE (filtered)\n",
    "freqs_pre_filt, psd_pre_filt = welch(pha_pre_filtered, fs=fs_pre_est, nperseg=len(pha_post_filtered), noverlap=0)\n",
    "idx_dom_pre_filt = np.argmax(psd_pre_filt)\n",
    "f_dom_pre_filt = freqs_pre_filt[idx_dom_pre_filt]\n",
    "P_dom_pre_filt = psd_pre_filt[idx_dom_pre_filt]\n",
    "# POST (filtered)\n",
    "freqs_post_filt, psd_post_filt = welch(pha_post_filtered,fs=fs_post_est,nperseg=len(pha_post_filtered), noverlap=0)\n",
    "\n",
    "idx_dom_post_filt = np.argmax(psd_post_filt)\n",
    "f_dom_post_filt = freqs_post_filt[idx_dom_post_filt]\n",
    "P_dom_post_filt = psd_post_filt[idx_dom_post_filt]\n",
    "# POST1 (filtered)\n",
    "freqs_post1_filt, psd_post1_filt = welch(pha_post_1_filtered, fs=fs_post1_est, nperseg=len(pha_post_filtered), noverlap=0)\n",
    "idx_dom_post1_filt = np.argmax(psd_post1_filt)\n",
    "f_dom_post1_filt = freqs_post1_filt[idx_dom_post1_filt]\n",
    "P_dom_post1_filt = psd_post1_filt[idx_dom_post1_filt]\n",
    "# POST2 (filtered)\n",
    "freqs_post2_filt, psd_post2_filt = welch(pha_post_2_filtered, fs=fs_post2_est, nperseg=len(pha_post_filtered), noverlap=0)\n",
    "idx_dom_post2_filt = np.argmax(psd_post2_filt)\n",
    "f_dom_post2_filt = freqs_post2_filt[idx_dom_post2_filt]\n",
    "P_dom_post2_filt = psd_post2_filt[idx_dom_post2_filt]\n",
    "\n",
    "# ----------- Print dominant frequencies (filtered) --------------\n",
    "print(f\"Dominant Frequency PRE (filtered):   f={f_dom_pre_filt:.3f} Hz | PSD={P_dom_pre_filt:.3f} °²/Hz\")\n",
    "print(f\"Dominant Frequency POST (filtered):  f={f_dom_post_filt:.3f} Hz | PSD={P_dom_post_filt:.3f} °²/Hz\")\n",
    "print(f\"Dominant Frequency POST1 (filtered): f={f_dom_post1_filt:.3f} Hz | PSD={P_dom_post1_filt:.3f} °²/Hz\")\n",
    "print(f\"Dominant Frequency POST2 (filtered): f={f_dom_post2_filt:.3f} Hz | PSD={P_dom_post2_filt:.3f} °²/Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12704aee",
   "metadata": {},
   "source": [
    "# 3. FRACTAL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945da7a",
   "metadata": {},
   "source": [
    "## 3.1 FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================\n",
    "# FRACTAL ANALYSIS FUNCTION\n",
    "#====================================================\n",
    "\n",
    "\n",
    "class fractal_analysis:\n",
    "#-------------------------------\n",
    "# Signal traitement function\n",
    "#-------------------------------\n",
    "    def fractal_dimension_boxcount(signal, time_vector=None):\n",
    "        signal_norm = (signal - np.min(signal)) / (np.max(signal) - np.min(signal) + 1e-10)\n",
    "        if time_vector is None:\n",
    "            time_vector = np.arange(len(signal))\n",
    "        time_norm = (time_vector - np.min(time_vector)) / (np.max(time_vector) - np.min(time_vector) + 1e-10)\n",
    "        n_points = len(signal)\n",
    "        min_boxes = 4\n",
    "        max_boxes = min(n_points // 2, 1000)\n",
    "        n_divisions = np.logspace(np.log10(min_boxes), np.log10(max_boxes), num=20, dtype=int)\n",
    "        n_divisions = np.unique(n_divisions)\n",
    "        box_counts = []\n",
    "        box_sizes = []\n",
    "        for n_div in n_divisions:\n",
    "            box_size = 1.0 / n_div\n",
    "            box_sizes.append(box_size)\n",
    "            box_indices_x = (time_norm / box_size).astype(int)\n",
    "            box_indices_y = (signal_norm / box_size).astype(int)\n",
    "            box_indices_x = np.clip(box_indices_x, 0, n_div - 1)\n",
    "            box_indices_y = np.clip(box_indices_y, 0, n_div - 1)\n",
    "            occupied_boxes = set(zip(box_indices_x, box_indices_y))\n",
    "            box_counts.append(len(occupied_boxes))\n",
    "        box_sizes = np.array(box_sizes)\n",
    "        box_counts = np.array(box_counts)\n",
    "        log_box_sizes = np.log(box_sizes)\n",
    "        log_counts = np.log(box_counts)\n",
    "        coeffs = np.polyfit(log_box_sizes, log_counts, 1)\n",
    "        D = -coeffs[0]\n",
    "        return D, box_sizes, box_counts\n",
    "\n",
    "    def dfa_analysis(signal, min_scale=4, max_scale=None, num_scales=20):\n",
    "        N = len(signal)\n",
    "        if max_scale is None:\n",
    "            max_scale = N // 4\n",
    "        signal_mean = np.mean(signal)\n",
    "        y = np.cumsum(signal - signal_mean)\n",
    "        scales = np.unique(np.logspace(np.log10(min_scale), np.log10(max_scale), num=num_scales, dtype=int))\n",
    "        fluctuations = []\n",
    "        for scale in scales:\n",
    "            num_segments = N // scale\n",
    "            if num_segments < 1:\n",
    "                continue\n",
    "            y_trimmed = y[:num_segments * scale]\n",
    "            segments = y_trimmed.reshape((num_segments, scale))\n",
    "            fluctuation_sum = 0\n",
    "            for segment in segments:\n",
    "                x_seg = np.arange(scale)\n",
    "                coeffs = np.polyfit(x_seg, segment, 1)\n",
    "                trend = np.polyval(coeffs, x_seg)\n",
    "                residuals = segment - trend\n",
    "                fluctuation_sum += np.sum(residuals ** 2)\n",
    "            F_n = np.sqrt(fluctuation_sum / (num_segments * scale))\n",
    "            fluctuations.append(F_n)\n",
    "        scales = np.array(scales[:len(fluctuations)])\n",
    "        fluctuations = np.array(fluctuations)\n",
    "        log_scales = np.log10(scales)\n",
    "        log_fluctuations = np.log10(fluctuations)\n",
    "        coeffs = np.polyfit(log_scales, log_fluctuations, 1)\n",
    "        alpha = coeffs[0]\n",
    "        return alpha, scales, fluctuations\n",
    "#-------------------------------\n",
    "# Plotting function\n",
    "#-------------------------------\n",
    "    def plot_box_counting_panel(signal_list, labels=None):\n",
    "        n = len(signal_list)\n",
    "        fig, axes = plt.subplots(1, n, figsize=(7*n, 5), squeeze=False)\n",
    "        for i, signal in enumerate(signal_list):\n",
    "            D, box_sizes, box_counts = fractal_analysis.fractal_dimension_boxcount(signal)\n",
    "            log_box_sizes = np.log(box_sizes)\n",
    "            log_counts = np.log(box_counts)\n",
    "            coeffs = np.polyfit(log_box_sizes, log_counts, 1)\n",
    "            ax = axes[0, i]\n",
    "            ax.scatter(log_box_sizes, log_counts, color=\"#028A88\", label='Data points')\n",
    "            ax.plot(log_box_sizes, np.polyval(coeffs, log_box_sizes), color=\"#F39EC7\", lw=2, label=f'D={D:.3f}')\n",
    "            title = labels[i] if labels else f\"Signal {i+1}\"\n",
    "            ax.set_title(f'{title} — Box Counting')\n",
    "            ax.set_xlabel('log(Box Size)')\n",
    "            ax.set_ylabel('log(Count)')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_dfa_panel(signal_list, labels=None, min_scale=4, max_scale=100):\n",
    "        n = len(signal_list)\n",
    "        fig, axes = plt.subplots(1, n, figsize=(7*n, 5), squeeze=False)\n",
    "        for i, signal in enumerate(signal_list):\n",
    "            alpha, scales, fluctuations = fractal_analysis.dfa_analysis(signal, min_scale=min_scale, max_scale=max_scale)\n",
    "            log_scales = np.log10(scales)\n",
    "            log_fluctuations = np.log10(fluctuations)\n",
    "            coeffs = np.polyfit(log_scales, log_fluctuations, 1)\n",
    "            ax = axes[0, i]\n",
    "            ax.scatter(log_scales, log_fluctuations, color=\"#028A88\", label='Data points')\n",
    "            ax.plot(log_scales, np.polyval(coeffs, log_scales), color=\"#F39EC7\", lw=2, label=f'α={alpha:.3f}')\n",
    "            title = labels[i] if labels else f\"Signal {i+1}\"\n",
    "            ax.set_title(f'{title} — DFA')\n",
    "            ax.set_xlabel('log(Window Size)')\n",
    "            ax.set_ylabel('log(Fluctuation)')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b5aae",
   "metadata": {},
   "source": [
    "## 3.2 PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504082e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. FRACTAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# -------- Box Counting Dimension ----------\n",
    "dim_pre, box_sizes_pre, box_counts_pre = fractal_analysis.fractal_dimension_boxcount(pha_pre_detrend_tapered)\n",
    "dim_post, box_sizes_post, box_counts_post = fractal_analysis.fractal_dimension_boxcount(pha_post_detrend_tapered)\n",
    "\n",
    "fractal_analysis.plot_box_counting_panel([pha_pre_detrend_tapered, pha_post_detrend_tapered], labels=[\"PRE\", \"POST\"])\n",
    "fractal_analysis.plot_box_counting_panel(\n",
    "    [pha_pre_detrend_tapered, pha_post_1_detrend_tapered, pha_post_2_filtered],\n",
    "    labels=[\"PRE\", \"POST_1\", \"POST_2\"]\n",
    ")\n",
    "\n",
    "# ------- DFA --------\n",
    "\n",
    "signals = [\n",
    "    (pha_pre_detrend_tapered, \"PRE\"),\n",
    "    (pha_post_1_detrend_tapered, \"POST_1\"),\n",
    "    (pha_post_2_filtered, \"POST_2\"),\n",
    "]\n",
    "\n",
    "for signal, label in signals:\n",
    "    N = len(signal)\n",
    "    signal_mean = np.mean(signal)\n",
    "\n",
    "    # 1. Integration (cumulative sum)\n",
    "    y = np.cumsum(signal - signal_mean)\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(y, color=\"#028A88\")\n",
    "    plt.title(f\"DFA Step 1: Integrated signal (cumsum) — {label}\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Integrated value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Window sizes (scales)\n",
    "    min_scale = 4\n",
    "    max_scale = N // 4\n",
    "    num_scales = 20\n",
    "    scales = np.unique(np.logspace(np.log10(min_scale), np.log10(max_scale), num=num_scales, dtype=int))\n",
    "\n",
    "    # 3. For a chosen scale, show detrending\n",
    "    chosen_scale = scales[len(scales)//2]\n",
    "    num_segments = N // chosen_scale\n",
    "    y_trimmed = y[:num_segments * chosen_scale]\n",
    "    segments = y_trimmed.reshape((num_segments, chosen_scale))\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    for i in range(min(3, num_segments)):\n",
    "        x_seg = np.arange(chosen_scale)\n",
    "        seg = segments[i]\n",
    "        coeffs = np.polyfit(x_seg, seg, 1)\n",
    "        trend = np.polyval(coeffs, x_seg)\n",
    "        plt.subplot(1,3,i+1)\n",
    "        plt.plot(x_seg, seg, label=\"Segment\")\n",
    "        plt.plot(x_seg, trend, '--', label=\"Local trend\")\n",
    "        plt.title(f\"{label} — Segment {i+1} (scale={chosen_scale})\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Compute RMS fluctuation for each scale\n",
    "    fluctuations = []\n",
    "    for scale in scales:\n",
    "        num_segments = N // scale\n",
    "        if num_segments < 1:\n",
    "            continue\n",
    "        y_trimmed = y[:num_segments * scale]\n",
    "        segments = y_trimmed.reshape((num_segments, scale))\n",
    "        fluctuation_sum = 0\n",
    "        for segment in segments:\n",
    "            x_seg = np.arange(scale)\n",
    "            coeffs = np.polyfit(x_seg, segment, 1)\n",
    "            trend = np.polyval(coeffs, x_seg)\n",
    "            residuals = segment - trend\n",
    "            fluctuation_sum += np.sum(residuals ** 2)\n",
    "        F_n = np.sqrt(fluctuation_sum / (num_segments * scale))\n",
    "        fluctuations.append(F_n)\n",
    "    scales = np.array(scales[:len(fluctuations)])\n",
    "    fluctuations = np.array(fluctuations)\n",
    "\n",
    "    # 5. Log-log regression\n",
    "    log_scales = np.log10(scales)\n",
    "    log_fluctuations = np.log10(fluctuations)\n",
    "    coeffs = np.polyfit(log_scales, log_fluctuations, 1)\n",
    "    alpha = coeffs[0]\n",
    "    fit_line = np.polyval(coeffs, log_scales)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(log_scales, log_fluctuations, color=\"#028A88\", label=\"Data points\")\n",
    "    plt.plot(log_scales, fit_line, color=\"#F39EC7\", lw=2, label=f'α={alpha:.3f}')\n",
    "    plt.title(f\"DFA Step 5: log(Fluctuation) vs log(Window size) — {label}\")\n",
    "    plt.xlabel(\"log10(Window size)\")\n",
    "    plt.ylabel(\"log10(Fluctuation)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Pente alpha estimée ({label}): {alpha:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbafb000",
   "metadata": {},
   "source": [
    "# ANNEXE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fe4e2",
   "metadata": {},
   "source": [
    "## ASSESSING REGULARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c34002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "is_nan = bia_post_2[\"PhA_48.8k_deg\"].isna()\n",
    "plt.plot(bia_post_2[\"time\"], is_nan, drawstyle=\"steps-post\", color=\"#d62728\")\n",
    "plt.title(\"Présence de NaN dans bia_post_2['PhA_48.8k_deg']\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"is NaN\")\n",
    "plt.yticks([0, 1], [\"Non\", \"Oui\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def check_sampling_regular(df, time_col=\"time\", plot=True, tol=0.01):\n",
    "    \"\"\"\n",
    "    Affiche la distribution des intervalles d'échantillonnage et vérifie la régularité.\n",
    "    Ajoute un diagnostic : faut-il resampler ?\n",
    "    tol : tolérance relative (ex: 0.01 = 1%) pour accepter l'irrégularité.\n",
    "    \"\"\"\n",
    "    t = pd.to_datetime(df[time_col], errors=\"coerce\").dropna()\n",
    "    dt = t.diff().dt.total_seconds().dropna()\n",
    "    print(f\"Nombre d'intervalles: {len(dt)}\")\n",
    "    print(f\"dt min: {dt.min():.6f} s, dt max: {dt.max():.6f} s, dt médian: {dt.median():.6f} s\")\n",
    "    print(f\"Écart-type des dt: {dt.std():.6f} s\")\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8,3))\n",
    "        plt.plot(dt.values, '.-', alpha=0.7)\n",
    "        plt.title(\"Intervalles d'échantillonnage (dt)\")\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(\"dt (s)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(6,3))\n",
    "        plt.hist(dt, bins=30, color=\"#028A88\", alpha=0.7)\n",
    "        plt.title(\"Distribution des dt\")\n",
    "        plt.xlabel(\"dt (s)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    # Diagnostic de régularité\n",
    "    rel_std = dt.std() / dt.median() if dt.median() != 0 else np.nan\n",
    "    if rel_std < tol:\n",
    "        print(f\"✅ Sampling régulier (écart-type relatif = {rel_std:.2%} < tolérance {tol:.2%}) → Pas besoin de resampler.\")\n",
    "        need_resample = False\n",
    "    else:\n",
    "        print(f\"⚠️ Sampling irrégulier (écart-type relatif = {rel_std:.2%} ≥ tolérance {tol:.2%}) → Il est conseillé de resampler.\")\n",
    "        need_resample = True\n",
    "    return dt, need_resample\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "dt_post2, need_resample_post2 = check_sampling_regular(bia_post_2)\n",
    "dt_post1,need_resample_post1 = check_sampling_regular(bia_post_1)\n",
    "dt_pre, need_resample_pre = check_sampling_regular(bia_pre)\n",
    "dt_post, need_resample_post = check_sampling_regular(bia_post)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
